{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9178f013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime, timezone, timedelta\n",
    "FIT_EPOCH_UTC = datetime(1989, 12, 31, tzinfo=timezone.utc)\n",
    "\n",
    "import gpxpy\n",
    "from garmin_fit_sdk import Decoder, Stream\n",
    "import folium\n",
    "import fiona\n",
    "import shapely\n",
    "from shapely.geometry import MultiLineString\n",
    "from haversine import haversine\n",
    "import calmap\n",
    "\n",
    "import pickle\n",
    "# m = folium.Map(location=(45.5236, -122.6750))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccd69db",
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_colors={'running': '#fc4c02',\n",
    "                 'cycling': '#00aa69',\n",
    "                 'AlpineSki': '#c860df',\n",
    "                 'NordicSki': '#eb3483',\n",
    "                 'hiking': '#e6e227',\n",
    "                 'IceSkate': '#68e8e8'}\n",
    "                 #'swimming': '#0060f5'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61ff27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data directory\n",
    "directory = 'C:/Users/imehr/Documents/GitHub/activities/export_7971201/activities/'\n",
    "os.chdir(path=directory)\n",
    "\n",
    "def extract_numeric(filename):\n",
    "    \"\"\"Extract numeric value from filename for sorting.\"\"\"\n",
    "    numbers = re.findall(r'\\d+', filename)\n",
    "    return int(numbers[0]) if numbers else float('inf')\n",
    "\n",
    "def factor_dimensions(total, max_diff=10):\n",
    "    \"\"\"Find optimal grid dimensions for subplot layout.\"\"\"\n",
    "    root = int(total**0.5)\n",
    "    for i in range(root, 0, -1):\n",
    "        if total % i == 0:\n",
    "            rows, cols = i, total // i\n",
    "            break\n",
    "    else:\n",
    "        rows, cols = 1, total\n",
    "\n",
    "    if abs(rows - cols) <= max_diff:\n",
    "        return rows, cols\n",
    "    \n",
    "    nearest_square_root = math.ceil(total**0.5)\n",
    "    return nearest_square_root, nearest_square_root\n",
    "\n",
    "def calculate_distance(points):\n",
    "    \"\"\"Calculate total distance from GPS points in miles.\"\"\"\n",
    "    total_distance = 0\n",
    "    for i in range(1, len(points)):\n",
    "        start = (points[i-1].latitude, points[i-1].longitude)\n",
    "        end = (points[i].latitude, points[i].longitude)\n",
    "        total_distance += haversine(start, end) * 0.621371\n",
    "    return total_distance\n",
    "\n",
    "def extract_date_distance(gpx):\n",
    "    \"\"\"Extract date and distance from GPX file.\"\"\"\n",
    "    first_point_time = gpx.tracks[0].segments[0].points[0].time\n",
    "    gpx_date = first_point_time.date() if first_point_time else None\n",
    "    \n",
    "    gpx_points = gpx.tracks[0].segments[0].points\n",
    "    total_distance = calculate_distance(gpx_points)\n",
    "    \n",
    "    return gpx_date, total_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387c4581",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Incremental load/update of activities_df using existing summary\n",
    "parquet_path = '../../activities.parquet'\n",
    "pickle_path = '../../activities.pkl'\n",
    "summary_csv_path = '../../activities_summary.csv'\n",
    "\n",
    "sorted_files = sorted(os.listdir(directory), key=extract_numeric)\n",
    "# Keep only supported filetypes\n",
    "sorted_files = [f for f in sorted_files if f.lower().endswith(('.gpx', '.fit'))]\n",
    "\n",
    "# Load existing full dataframe if present\n",
    "existing_df = None\n",
    "if os.path.exists(parquet_path):\n",
    "    try:\n",
    "        existing_df = pd.read_parquet(parquet_path)\n",
    "    except Exception:\n",
    "        try:\n",
    "            existing_df = pd.read_parquet(parquet_path, engine='fastparquet')\n",
    "        except Exception:\n",
    "            existing_df = None\n",
    "if existing_df is None and os.path.exists(pickle_path):\n",
    "    try:\n",
    "        existing_df = pd.read_pickle(pickle_path)\n",
    "    except Exception:\n",
    "        existing_df = None\n",
    "if existing_df is not None and not existing_df.empty:\n",
    "    existing_df['date'] = pd.to_datetime(existing_df['date'])\n",
    "\n",
    "# Determine processed files from summary CSV if available, else from existing_df\n",
    "if os.path.exists(summary_csv_path):\n",
    "    try:\n",
    "        summary_df = pd.read_csv(summary_csv_path)\n",
    "        processed_files = set(summary_df['file'].astype(str))\n",
    "    except Exception:\n",
    "        processed_files = set(existing_df['file'].astype(str)) if existing_df is not None and not existing_df.empty else set()\n",
    "else:\n",
    "    processed_files = set(existing_df['file'].astype(str)) if existing_df is not None and not existing_df.empty else set()\n",
    "\n",
    "new_files = [f for f in sorted_files if f not in processed_files]\n",
    "\n",
    "# Process only new files\n",
    "records = []\n",
    "sample_step = 10\n",
    "for file in tqdm(new_files, desc='Processing new activities'):\n",
    "    if file.lower().endswith('.gpx'):\n",
    "        with open(file) as f:\n",
    "            gpx = gpxpy.parse(f)\n",
    "        track = gpx.tracks[0]\n",
    "        segment = track.segments[0]\n",
    "        all_points = segment.points\n",
    "\n",
    "        sampled_points = all_points[::sample_step]\n",
    "        latlons = [(p.latitude, p.longitude) for p in sampled_points]\n",
    "        elevations_list = [p.elevation for p in sampled_points]\n",
    "\n",
    "        # activity time\n",
    "        time_list = [p.time for p in sampled_points]\n",
    "        # Calculate total time of the activity in minutes\n",
    "        if time_list and time_list[0] is not None and time_list[-1] is not None:\n",
    "            total_time_minutes = (time_list[-1] - time_list[0]).total_seconds() / 60.0\n",
    "        else:\n",
    "            total_time_minutes = 0.0\n",
    "\n",
    "        # Cumulative distances in km\n",
    "        cum_dist_km = [0.0]\n",
    "        for j in range(1, len(sampled_points)):\n",
    "            d = haversine(\n",
    "                (sampled_points[j-1].latitude, sampled_points[j-1].longitude),\n",
    "                (sampled_points[j].latitude, sampled_points[j].longitude)\n",
    "            )\n",
    "            cum_dist_km.append(cum_dist_km[-1] + d)\n",
    "\n",
    "        activity_date, total_distance_miles = extract_date_distance(gpx)\n",
    "        activity_type = track.type\n",
    "\n",
    "        # Geometry via Fiona (best-effort)\n",
    "        try:\n",
    "            with fiona.open(file, 'r', layer='tracks') as records_iter:\n",
    "                geoms = [shapely.geometry.shape(rec['geometry']) for rec in records_iter]\n",
    "                geometry = geoms[0] if len(geoms) > 0 else None\n",
    "        except Exception:\n",
    "            geometry = None\n",
    "\n",
    "    elif file.lower().endswith('.fit'):\n",
    "        # Parse FIT with garmin_fit_sdk\n",
    "        activity_date = None\n",
    "        activity_type = None\n",
    "        total_time_minutes = 0.0\n",
    "        total_distance_miles = 0.0\n",
    "        latlons = []\n",
    "        elevations_list = []\n",
    "        cum_dist_km = []\n",
    "        geometry = None\n",
    "\n",
    "        try:\n",
    "            stream = Stream.from_file(file)\n",
    "            decoder = Decoder(stream)\n",
    "            messages, errors = decoder.read()\n",
    "\n",
    "            # Derive local start datetime\n",
    "            local_dt = None\n",
    "            # Try activity messages first (often include local_timestamp)\n",
    "            for act in messages.get('activity_mesgs', []):\n",
    "                if 'local_timestamp' in act and act['local_timestamp'] is not None:\n",
    "                    try:\n",
    "                        # Convert from FIT epoch (1989-12-31 UTC) to UTC, then to local\n",
    "                        lt = int(act['local_timestamp'])\n",
    "                        utc_dt = FIT_EPOCH_UTC + timedelta(seconds=lt)\n",
    "                        local_dt = utc_dt.astimezone()\n",
    "                        break\n",
    "                    except Exception:\n",
    "                        local_dt = None\n",
    "                if local_dt is None and 'timestamp' in act and act['timestamp'] is not None:\n",
    "                    try:\n",
    "                        ts = act['timestamp']\n",
    "                        if getattr(ts, 'tzinfo', None) is None:\n",
    "                            ts = ts.replace(tzinfo=timezone.utc)\n",
    "                        local_dt = ts.astimezone()\n",
    "                        break\n",
    "                    except Exception:\n",
    "                        local_dt = None\n",
    "\n",
    "            # Fallback to session start_time if needed\n",
    "            if local_dt is None:\n",
    "                for sess in messages.get('session_mesgs', []):\n",
    "                    if 'start_time' in sess and sess['start_time'] is not None:\n",
    "                        try:\n",
    "                            ts = sess['start_time']\n",
    "                            if getattr(ts, 'tzinfo', None) is None:\n",
    "                                ts = ts.replace(tzinfo=timezone.utc)\n",
    "                            local_dt = ts.astimezone()\n",
    "                            break\n",
    "                        except Exception:\n",
    "                            local_dt = None\n",
    "\n",
    "            if activity_date is None and local_dt is not None:\n",
    "                activity_date = pd.to_datetime(local_dt).date()\n",
    "\n",
    "            # Extract sport/sub_sport and distance from session messages\n",
    "            for sess in messages.get('session_mesgs', []):\n",
    "                if activity_type is None:\n",
    "                    if 'sub_sport' in sess and sess['sub_sport'] is not None:\n",
    "                        activity_type = str(sess['sub_sport'])\n",
    "                    elif 'sport' in sess and sess['sport'] is not None:\n",
    "                        activity_type = str(sess['sport'])\n",
    "                if 'total_distance' in sess and sess['total_distance'] is not None:\n",
    "                    total_distance_miles = float(sess['total_distance']) / 1609.344\n",
    "                if 'total_elapsed_time' in sess and sess['total_elapsed_time'] is not None:\n",
    "                    total_time_minutes = float(sess['total_elapsed_time']) / 60.0\n",
    "                else:\n",
    "                    total_time_minutes = 0.0\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        if activity_date is None:\n",
    "            # Fallback from file modified time\n",
    "            activity_date = pd.to_datetime(os.path.getmtime(file), unit='s').date()\n",
    "        if activity_type is None:\n",
    "            activity_type = 'other'\n",
    "\n",
    "    records.append({\n",
    "        'file': file,\n",
    "        'date': activity_date,\n",
    "        'activity_type': activity_type,\n",
    "        'total_time_minutes': total_time_minutes,\n",
    "        'total_distance_miles': total_distance_miles,\n",
    "        'points': latlons,\n",
    "        'elevations': elevations_list,\n",
    "        'distances_km': cum_dist_km,\n",
    "        'geometry': geometry,\n",
    "    })\n",
    "\n",
    "new_df = pd.DataFrame.from_records(records)\n",
    "if not new_df.empty:\n",
    "    new_df['date'] = pd.to_datetime(new_df['date'])\n",
    "\n",
    "# Merge with existing\n",
    "if existing_df is not None and not existing_df.empty:\n",
    "    activities_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "else:\n",
    "    activities_df = new_df.copy() if not new_df.empty else pd.DataFrame(columns=['file','date','activity_type','total_time_minutes','total_distance_miles','points','elevations','distances_km','geometry'])\n",
    "\n",
    "# Export updated dataframe and summary\n",
    "if not activities_df.empty:\n",
    "    try:\n",
    "        activities_df.to_parquet(parquet_path, index=False)\n",
    "    except Exception:\n",
    "        try:\n",
    "            activities_df.to_parquet(parquet_path, index=False, engine='fastparquet')\n",
    "        except Exception:\n",
    "            activities_df.to_pickle(pickle_path)\n",
    "\n",
    "    summary_cols = ['file', 'date', 'activity_type', 'total_time_minutes', 'total_distance_miles', 'points', 'elevations']\n",
    "    activities_df.to_csv(summary_csv_path, index=False, columns=summary_cols)\n",
    "\n",
    "# sort by activity date\n",
    "activities_df = activities_df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "# Grid dimensions based on activities count\n",
    "activities_with_gps = activities_df[activities_df['geometry'].notnull() & activities_df['geometry'].apply(lambda g: hasattr(g, 'geoms') and len(g.geoms) > 0)]\n",
    "num_tracks = len(activities_with_gps)\n",
    "dimensions = factor_dimensions(max(num_tracks, 1))\n",
    "n_rows, n_cols = dimensions[0], dimensions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9d83e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### HEATMAP\n",
    "\n",
    "m = folium.Map(tiles=\"Cartodb dark_matter\")\n",
    "\n",
    "for _, row in tqdm(activities_with_gps.iterrows(), total=len(activities_with_gps), desc=\"Creating Heatmap\"):\n",
    "    points = row['points']\n",
    "    color = activity_colors.get(row['activity_type'], '#000000')\n",
    "    if points and len(points) > 1:\n",
    "        folium.PolyLine(points, color=color, opacity=0.5).add_to(m)\n",
    "\n",
    "m.fit_bounds(m.get_bounds())\n",
    "m.save('../../figures/heatmap.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8a3f67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### TRACK GRID\n",
    "\n",
    "multilines = [geom for geom in activities_with_gps['geometry'] if geom is not None]\n",
    "print(f'Multilines Extraction Complete: {len(multilines)} Multilines')\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols*0.67, n_rows*0.67))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, multi_line in tqdm(list(enumerate(multilines)), desc=\"Plotting Tracks\", total=len(multilines)):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    for line in multi_line.geoms:\n",
    "        x, y = line.xy\n",
    "        ax.plot(x, y, color='black', lw=0.75)\n",
    "    \n",
    "    # Set bounds with small padding\n",
    "    minx, miny, maxx, maxy = multi_line.bounds\n",
    "    ax.set_xlim(minx - 0.001, maxx + 0.001)\n",
    "    ax.set_ylim(miny - 0.001, maxy + 0.001)\n",
    "    \n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.spines[:].set_visible(False)\n",
    "       \n",
    "# Hide unused subplots\n",
    "for j in range(len(multilines), len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout(pad=0.1, w_pad=0.25, h_pad=0.1)\n",
    "plt.savefig('../../figures/tracks.png', format='png', dpi=300, transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701eaca6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### ELEVATION OVERLAY\n",
    "\n",
    "num_points = 1000\n",
    "x_values = np.linspace(0, 1, num_points)\n",
    "\n",
    "max_elevation = activities_with_gps['elevations'].dropna().map(lambda arr: max(arr) if len(arr) else 0).max()\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for _, row in tqdm(activities_with_gps.iterrows(), total=len(activities_with_gps), desc=\"Creating Elevation Overlay\"):\n",
    "    distance = row['distances_km']\n",
    "    elevation = row['elevations']\n",
    "    if not distance or not elevation or len(distance) != len(elevation):\n",
    "        continue\n",
    "    scaled_distances = [d / distance[-1] if distance[-1] else 0 for d in distance]\n",
    "    interpolated_elevations = np.interp(x_values, scaled_distances, elevation)\n",
    "\n",
    "    plt.plot(x_values, interpolated_elevations, color='black', linestyle='-', linewidth=0.1, alpha=0.1)\n",
    "    plt.fill_between(x_values, 0, interpolated_elevations, color='black', alpha=0.05)\n",
    "\n",
    "plt.ylim(0, max_elevation)\n",
    "plt.grid(False)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "for spine in plt.gca().spines.values():\n",
    "    spine.set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../figures/elevation_overlay.png', format='png', dpi=300, transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bddf2c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### ELEVATION GRID\n",
    "\n",
    "max_points = 50\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols*0.67, n_rows*0.67))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (_, row) in tqdm(list(enumerate(activities_with_gps.iterrows())), total=len(activities_with_gps), desc=\"Creating Elevation Grid\"):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    distance = row['distances_km']\n",
    "    elevation = row['elevations']\n",
    "    if not distance or not elevation:\n",
    "        ax.axis('off')\n",
    "        continue\n",
    "    \n",
    "    norm_elevations = [(elev - min(elevation)) / (max(elevation) - min(elevation)) if (max(elevation) - min(elevation)) else 0 for elev in elevation]\n",
    "    \n",
    "    if len(distance) > max_points:\n",
    "        indices = np.linspace(0, len(distance) - 1, max_points).astype(int)\n",
    "        distance = [distance[idx] for idx in indices]\n",
    "        norm_elevations = [norm_elevations[idx] for idx in indices]\n",
    "    \n",
    "    ax.plot(distance, norm_elevations, color='black', lw=0.75)\n",
    "        \n",
    "    ax.set_xlim(-0.5, distance[-1])\n",
    "    ax.set_ylim(-0.5, 1.5)\n",
    "    \n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.spines[:].set_visible(False)\n",
    "\n",
    "# Hide unused subplots\n",
    "for j in range(len(activities_with_gps), len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout(pad=0.1, w_pad=0.25, h_pad=0.1)\n",
    "plt.savefig('../../figures/elevation_grid.png', format='png', dpi=300, transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af3303e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### ACTIVITY CALENDAR\n",
    "\n",
    "date_distance_series = activities_df.groupby('date')['total_distance_miles'].sum().sort_index()\n",
    "\n",
    "calendar = calmap.calendarplot(\n",
    "    np.log1p(date_distance_series), \n",
    "    cmap=plt.cm.Oranges, \n",
    "    fig_kws=dict(figsize=(20, 10)), \n",
    "    yearlabel_kws={'color': 'lightgray'}\n",
    ")\n",
    "calendar[0].savefig('../../figures/activity_calendar.png', format='png', dpi=300, transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6c7e0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### WEEKLY MILES\n",
    "\n",
    "weekly_data = activities_with_gps.set_index('date')['total_distance_miles'].resample('W').sum()\n",
    "years = weekly_data.index.year.unique()\n",
    "fig, axes = plt.subplots(nrows=len(years), ncols=1, figsize=(20, 16), sharey=True)\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    ax = axes[i]\n",
    "    full_year_range = pd.date_range(start=f'{year}-01-01', end=f'{year}-12-31', freq='W-SUN')\n",
    "    year_data = weekly_data[weekly_data.index.year == year].reindex(full_year_range)\n",
    "    \n",
    "    ax.plot(year_data.index, year_data.values, color='#fc4c02', marker='o', markerfacecolor='white', label=f\"{year}\")\n",
    "    ax.fill_between(year_data.index, 0, year_data.values, where=~year_data.isna(), color='#fc4c02', alpha=0.2)\n",
    "    \n",
    "    ax.set_xlim(pd.Timestamp(min(year_data.index)), pd.Timestamp(f'{year}-12-31'))\n",
    "    ax.set_ylim(0, max(weekly_data.values) * 1.1)\n",
    "    ax.set_ylabel(year, fontsize=16)\n",
    "    ax.xaxis.set_major_locator(mdates.MonthLocator(bymonthday=7))\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%b'))\n",
    "\n",
    "plt.suptitle('Weekly Distance', fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../figures/weekly_distance.png', format='png', dpi=100, transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447a5cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MONTHLY MILES\n",
    "\n",
    "monthly_data = activities_with_gps.set_index('date')['total_distance_miles'].resample('M').sum()\n",
    "years = monthly_data.index.year.unique()\n",
    "fig, axes = plt.subplots(nrows=len(years), ncols=1, figsize=(10, 8), sharey=True)\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    ax = axes[i]\n",
    "    year_data = monthly_data[monthly_data.index.year == year]\n",
    "    year_data.index = year_data.index.to_period('M').to_timestamp()\n",
    "    \n",
    "    ax.plot(year_data.index, year_data.values, color='#fc4c02', label=f\"{year}\", marker='o', markerfacecolor='white')\n",
    "    ax.fill_between(year_data.index, 0, year_data.values, color='#fc4c02', alpha=0.2)\n",
    "    \n",
    "    ax.set_xlim(pd.Timestamp(f'{year}-01-01'), pd.Timestamp(f'{year}-12-01'))\n",
    "    ax.set_ylim(0, max(monthly_data.values) * 1.1)\n",
    "    ax.set_ylabel(year, fontsize=14)\n",
    "    \n",
    "    ax.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%b'))\n",
    "\n",
    "plt.suptitle('Monthly Distance', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../figures/monthly_distance.png', format='png', dpi=100, transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e651594",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### MONTHLY MILE TYPE\n",
    "\n",
    "monthly_data = activities_with_gps.groupby(['activity_type', pd.Grouper(key='date', freq='M')])['total_distance_miles'].sum()\n",
    "monthly_data = monthly_data.reset_index()\n",
    "monthly_pivot = monthly_data.pivot_table(index='date', columns='activity_type', values='total_distance_miles', fill_value=0)\n",
    "\n",
    "years = monthly_pivot.index.year.unique()\n",
    "fig, axes = plt.subplots(nrows=len(years), ncols=1, figsize=(10, 12), sharey=True)\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    ax = axes[i]\n",
    "    year_data = monthly_pivot[monthly_pivot.index.year == year]\n",
    "    year_data.index = year_data.index.to_period('M').to_timestamp()\n",
    "    \n",
    "    activities = year_data.columns\n",
    "    month_indexes = year_data.index\n",
    "    stacked_distances = [year_data[activity].values for activity in activities]\n",
    "    activity_colors_list = [activity_colors.get(activity, '#000000') for activity in activities]\n",
    "    \n",
    "    ax.stackplot(month_indexes, stacked_distances, labels=activities, colors=activity_colors_list, alpha=0.7)\n",
    "    \n",
    "    total_distance = year_data.sum(axis=1)\n",
    "    ax.plot(month_indexes, total_distance, color='black', marker='o', markerfacecolor='white', lw=1, label=\"Total\")\n",
    "    \n",
    "    ax.set_xlim(pd.Timestamp(f'{year}-01-01'), pd.Timestamp(f'{year}-12-01'))\n",
    "    ax.set_ylim(0, max(monthly_pivot.sum(axis=1)) * 1.1)\n",
    "    ax.set_ylabel(year, fontsize=12)\n",
    "    \n",
    "    ax.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%b'))\n",
    "\n",
    "    if i == 0:\n",
    "        ax.legend(loc='upper left', fontsize=8)\n",
    "\n",
    "plt.suptitle('Monthly Activity Distance by Type', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../figures/monthly_distance_split.png', format='png', dpi=100, transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46566568",
   "metadata": {},
   "outputs": [],
   "source": [
    "### WEEKLY CUMULATIVE SUM\n",
    "\n",
    "# --- GROUP Weekly ---\n",
    "weekly_data = (\n",
    "    activities_with_gps\n",
    "    .groupby(['activity_type', pd.Grouper(key='date', freq='W')])['total_distance_miles']\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "weekly_pivot = weekly_data.pivot_table(\n",
    "    index='date',\n",
    "    columns='activity_type',\n",
    "    values='total_distance_miles',\n",
    "    fill_value=0\n",
    ").sort_index()\n",
    "\n",
    "# --- For each year: ensure all days Jan 1 to Dec 31, fill missing with 0, then cumulative ---\n",
    "max_date = weekly_pivot.index.max()\n",
    "cumulative_list = []\n",
    "for year in weekly_pivot.index.year.unique():\n",
    "    # Only fill up to the last day with data, but always include Dec 31st\n",
    "    dec_31 = pd.Timestamp(f'{year}-12-31')\n",
    "    end_day = min(max_date, dec_31)\n",
    "    full_index = pd.date_range(start=f'{year}-01-01', end=end_day, freq='W')\n",
    "    group = weekly_pivot[weekly_pivot.index.year == year].reindex(full_index).fillna(0)\n",
    "    group = group.cumsum()\n",
    "    cumulative_list.append(group)\n",
    "\n",
    "cumulative_pivot = pd.concat(cumulative_list)\n",
    "\n",
    "# --- PLOT ---\n",
    "years = cumulative_pivot.index.year.unique()\n",
    "fig, axes = plt.subplots(nrows=len(years), ncols=1, figsize=(10, 12), sharey=True)\n",
    "if len(years) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    ax = axes[i]\n",
    "    year_data = cumulative_pivot[cumulative_pivot.index.year == year]\n",
    "\n",
    "    activities = year_data.columns\n",
    "    week_indexes = year_data.index\n",
    "    stacked_distances = [year_data[activity].values for activity in activities]\n",
    "    activity_colors_list = [activity_colors.get(activity, '#000000') for activity in activities]\n",
    "\n",
    "    # STACKED CUMULATIVE AREA\n",
    "    ax.stackplot(\n",
    "        week_indexes,\n",
    "        stacked_distances,\n",
    "        labels=activities,\n",
    "        colors=activity_colors_list,\n",
    "        alpha=0.7\n",
    "    )\n",
    "\n",
    "    # TOTAL CUMULATIVE DISTANCE LINE\n",
    "    total_distance = year_data.sum(axis=1)\n",
    "    ax.plot(\n",
    "        week_indexes,\n",
    "        total_distance,\n",
    "        color='black',\n",
    "        lw=1,\n",
    "        label=\"Total\"\n",
    "    )\n",
    "\n",
    "    # Black dots: first of each month with data, plus Dec 31st\n",
    "    month_starts = pd.date_range(start=f'{year}-01-01', end=f'{year}-12-31', freq='MS')\n",
    "    dec_31 = pd.Timestamp(f'{year}-12-31')\n",
    "    dot_dates = list(month_starts)\n",
    "    if dec_31 not in dot_dates:\n",
    "        dot_dates.append(dec_31)\n",
    "    dot_dates = [d for d in dot_dates if d <= end_day]\n",
    "\n",
    "    # Interpolate y-values for dots if date not in index\n",
    "    dots_y = []\n",
    "    for d in dot_dates:\n",
    "        if d in total_distance.index:\n",
    "            dots_y.append(total_distance.loc[d])\n",
    "        else:\n",
    "            # Interpolate if not present\n",
    "            dots_y.append(total_distance.reindex(total_distance.index.union([d])).sort_index().interpolate(method='time').loc[d])\n",
    "\n",
    "    ax.plot(\n",
    "        dot_dates,\n",
    "        dots_y,\n",
    "        color='black',\n",
    "        marker='o',\n",
    "        markerfacecolor='white',\n",
    "        linestyle='None'\n",
    "    )\n",
    "\n",
    "    ax.set_xlim(pd.Timestamp(f'{year}-01-01'), pd.Timestamp(f'{year}-12-31'))\n",
    "    ax.set_ylim(0, total_distance.max() * 1.1)\n",
    "    ax.set_ylabel(year, fontsize=12)\n",
    "\n",
    "    ax.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%b'))\n",
    "\n",
    "    if i == 0:\n",
    "        ax.legend(loc='upper left', fontsize=8)\n",
    "\n",
    "plt.suptitle('Cumulative Activity Distance by Type (Weekly)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../figures/cumulative_distance_split.png', format='png', dpi=100, transparent=True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
